{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.close('all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]=\"3\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For example, here's several helpful packages to load in \n",
    "\n",
    "\n",
    "#from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D, Input\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: ((28, 28), ()), types: (tf.float64, tf.uint8)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import tensorflow as tf \n",
    "\n",
    "train, test = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "images, labels = train\n",
    "images = images/255\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Convolutional Neural Network\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "\n",
    "import tensorflow as tf \n",
    "\n",
    "# Import modules\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "\n",
    "visible = Input(shape=(28,28,1))\n",
    "conv1 = Conv2D(32, kernel_size=3, activation='relu')(visible)\n",
    "pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "conv2 = Conv2D(64, kernel_size=3, activation='relu')(pool1)\n",
    "pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "conv3 = Conv2D(64, kernel_size=3, activation='relu')(pool2)\n",
    "pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "#conv4 = Conv2D(16, kernel_size=4, activation='relu')(pool3)\n",
    "#pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "#placeHolder1 = tf.placeholder(tf.float32,shape=(None, 800)) \n",
    "flat = Flatten()(pool3) \n",
    "\n",
    "\n",
    "\n",
    "# try\n",
    "\n",
    "#flat_num = np.array(flat)\n",
    "\n",
    "\n",
    "\n",
    "#flat =flat_num\n",
    "\n",
    "#print(type(flat))\n",
    "###########################################################################\n",
    "\n",
    "#sess=tf.Session()\n",
    "#tf.InteractiveSession()  # run an interactive session in Tf.\n",
    "\n",
    "#res=sess.run(flat)\n",
    "#print(res)\n",
    "\n",
    "#sess = tf.compat.v1.Session()\n",
    "\n",
    "\n",
    "###########################################################################\n",
    "#sess = tf.Session()\n",
    "#tf.InteractiveSession()  # run an interactive session in Tf.\n",
    " \n",
    "#flat= tf.stack([flat])\n",
    "X=flat\n",
    "#flat_np = flat.eval()\n",
    "#print(type(flat_np))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation\n",
    "def forward_prop(params):\n",
    "    \"\"\"Forward propagation as objective function\n",
    "\n",
    "    This computes for the forward propagation of the neural network, as\n",
    "    well as the loss. It receives a set of parameters that must be\n",
    "    rolled-back into the corresponding weights and biases.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    params: np.ndarray\n",
    "        The dimensions should include an unrolled version of the\n",
    "        weights and biases.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The computed negative log-likelihood loss given the parameters\n",
    "    \"\"\"\n",
    "    # Neural network architecture\n",
    "    n_inputs = 10\n",
    "    n_hidden = 20\n",
    "    #n_hidden_2 = 1000\n",
    "    n_classes = 10\n",
    "\n",
    "    # Roll-back the weights and biases\n",
    "    W1 = params[0:20].reshape((n_inputs,n_hidden))\n",
    "    b1 = params[20:220].reshape((n_hidden,))\n",
    "    W2 = params[220:420].reshape((n_hidden,n_classes,))\n",
    "    b2 = params[420:430].reshape((n_classes,))\n",
    "\n",
    "    X=flat\n",
    "   # X=X.eval()\n",
    "\n",
    "\n",
    "    #print(W1.shape)\n",
    "    #print(X.shape)\n",
    "    #X = K.placeholder(shape=(2, 3))\n",
    "    #W1 = K.placeholder(shape=(1600, 2000))\n",
    "    #F = K.dot(X, W1)  \n",
    "    \n",
    "    #X = tf.placeholder(shape=[X, None],dtype=tf.float32)\n",
    "    #W1 = tf.placeholder(shape=[W1, None],dtype=tf.float32)\n",
    "    #a1 = tf.placeholder(shape=[a1, None],dtype=tf.float32)\n",
    "    #W2 = tf.placeholder(shape=[W2, None],dtype=tf.float32)\n",
    "    #b1 = tf.placeholder(shape=[b1, None],dtype=tf.float32)\n",
    "    #b2 = tf.placeholder(shape=[b2, None],dtype=tf.float32)\n",
    "\n",
    "    # Perform forward propagation\n",
    "    #z1 = tf.tensordot(X,W1,axes = 1) + b1  # Pre-activation in Layer 1\n",
    "    #a1 = np.tanh(z1)     # Activation in Layer 1\n",
    "    #z2 = tf.tensordot(a1,W2,axes = 1) + b2 # Pre-activation in Layer 2\n",
    "    \n",
    "    \n",
    "    X = K.placeholder(shape=(10, 10))\n",
    "    W1 = K.placeholder(shape=(10, 20))\n",
    "    z1 = K.dot(X, W1)+b1\n",
    "    \n",
    "    \n",
    "    \n",
    "    def tanh(z1):\n",
    "        return np.tanh(z1)\n",
    "    \n",
    "    #keras.backend.tanh(z1)\n",
    "    tf.keras.backend.tanh(z1)\n",
    "    keras.activations.tanh(z1)\n",
    "    \n",
    "    \n",
    "     # Perform forward propagation\n",
    "    #z1 = X.dot(W1) + b1  # Pre-activation in Layer 1\n",
    "    #a1 = np.tanh(z1)     # Activation in Layer 1\n",
    "    a1 = tf.keras.backend.tanh(z1)    # Activation in Layer 1\n",
    "    \n",
    "    \n",
    "    a1 = K.placeholder(shape=(10,20))\n",
    "    W2 = K.placeholder(shape=(20, 10))\n",
    "    z2 = K.dot(a1, W2)+b2\n",
    "    \n",
    "    \n",
    "    #z2 = a1.dot(W2) + b2 # Pre-activation in Layer 2\n",
    "    logits = z2          # Logits for Layer 2\n",
    "    \n",
    "    # Compute for the softmax of the logits\n",
    "    exp_scores = tf.keras.backend.exp(logits)\n",
    "    #exp_scores = np.exp(logits)    # np.exp means exponential function\n",
    "    \n",
    "  \n",
    "    \n",
    "    probs = exp_scores / tf.keras.backend.sum(exp_scores, axis=None, keepdims=True)   \n",
    "\n",
    "    # Compute for the negative log likelihood\n",
    "    N = 10 # Number of samples\n",
    "    #corect_logprobs = tf.keras.backend.log(probs[range(N), y])\n",
    "    corect_logprobs = tf.keras.backend.log(probs)\n",
    "    loss = (tf.keras.backend.sum(corect_logprobs, axis=None, keepdims=True)) / N\n",
    "    \n",
    "    \n",
    "    # Perform forward propagation\n",
    "    #z1 = X.dot(W1) + b1  # Pre-activation in Layer 1\n",
    "    #a1 = np.tanh(z1)     # Activation in Layer 1\n",
    "    #z2 = a1.dot(W2) + b2 # Pre-activation in Layer 2\n",
    "    #logits = z2          # Logits for Layer 2\n",
    "\n",
    "    # Compute for the softmax of the logits\n",
    "    #exp_scores = np.exp(logits)    # np.exp means exponential function\n",
    "    #probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)   #axis=1 means columns\n",
    "    # Compute for the negative log likelihood\n",
    "    #N = 10 # Number of samples\n",
    "    #corect_logprobs = -np.log(probs[range(N), y])\n",
    "    #loss = np.sum(corect_logprobs) / N\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    \"\"\"Higher-level method to do forward_prop in the\n",
    "    whole swarm.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    x: numpy.ndarray of shape (n_particles, dimensions)\n",
    "        The swarm that will perform the search\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray of shape (n_particles, )\n",
    "        The computed loss for each particle\n",
    "    \"\"\"\n",
    "    n_particles = x.shape[0]\n",
    "    j = [forward_prop(x[i]) for i in range(n_particles)]\n",
    "    #original\n",
    "    #return np.array(j)\n",
    "    #try\n",
    "    return tf.array(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyswarms as ps\n",
    "import keras\n",
    "from pyswarms.utils.functions import single_obj as fx\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "\n",
    "%time\n",
    "\n",
    "# Initialize swarm\n",
    "options = {'c1': 0.5, 'c2': 0.3, 'w':0.9}\n",
    "\n",
    "# Call instance of PSO\n",
    "dimensions = ( 10 * 20) + (20 * 10) + 20 + 10\n",
    "optimizer = ps.single.GlobalBestPSO(n_particles=5, dimensions=dimensions, options=options)\n",
    "\n",
    "# Perform optimization\n",
    "cost, pos = optimizer.optimize(f, iters=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, pos):\n",
    "    \"\"\"\n",
    "    Use the trained weights to perform class predictions.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    X: numpy.ndarray\n",
    "        Input Iris dataset\n",
    "    pos: numpy.ndarray\n",
    "        Position matrix found by the swarm. Will be rolled\n",
    "        into weights and biases.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Neural network architecture\n",
    "    n_inputs = 10\n",
    "    n_hidden = 20\n",
    "    #n_hidden_2 = 1000\n",
    "    n_classes = 10\n",
    "\n",
    "    # Roll-back the weights and biases\n",
    "    W1 = params[0:20].reshape((n_inputs,n_hidden))\n",
    "    b1 = params[20:220].reshape((n_hidden,))\n",
    "    W2 = params[220:420].reshape((n_hidden,n_classes,))\n",
    "    b2 = params[420:430].reshape((n_classes,))\n",
    "    \n",
    "    X=flat\n",
    "    \n",
    "    X = K.placeholder(shape=(10, 10))\n",
    "    W1 = K.placeholder(shape=(10, 2000))\n",
    "    z1 = K.dot(X, W1)+b1\n",
    "    \n",
    "    \n",
    "    \n",
    "    def tanh(z1):\n",
    "        return np.tanh(z1)\n",
    "    \n",
    "    #keras.backend.tanh(z1)\n",
    "    tf.keras.backend.tanh(z1)\n",
    "    keras.activations.tanh(z1)\n",
    "    \n",
    "    \n",
    "     # Perform forward propagation\n",
    "    #z1 = X.dot(W1) + b1  # Pre-activation in Layer 1\n",
    "    #a1 = np.tanh(z1)     # Activation in Layer 1\n",
    "    a1 = tf.keras.backend.tanh(z1)    # Activation in Layer 1\n",
    "    \n",
    "    \n",
    "    a1 = K.placeholder(shape=(10,2000 ))\n",
    "    W2 = K.placeholder(shape=(2000, 10))\n",
    "    z2 = K.dot(a1, W2)+b2\n",
    "    \n",
    "    \n",
    "    logits = z2          # Logits for Layer 2\n",
    "\n",
    "    y_pred = np.argmax(logits, axis=1)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
